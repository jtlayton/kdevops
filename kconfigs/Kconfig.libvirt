config LIBVIRT_INSTALL
	bool "Install libvirt"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the Ansible role which installs
	  libvirt for you will be run. The goal will be to ensure you have
	  libvirt installed and running.

config LIBVIRT_CONFIGURE
	bool "Configure libvirt so you spawn guests as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  If this option is enabled then the Ansible role which configures
	  libvirt for you will be run. This typically just requires adding the
	  user to a specific set of groups. The user must log out and back
	  in again, to ensure the new group takes effect. The goal in the
	  configuration will be to ensure you can use libvirt to spawn guests
	  as a regular user. You are encouraged to say y here unless you know
	  what you are doing or you already know this works. If you are unsure,
	  the litmus test for this is if you can run vagrant up, on any public
	  demo box available.

config LIBVIRT_VERIFY
	bool "Verify that a user can spawn libvirt as a regular user"
	default y if KDEVOPS_FIRST_RUN
	default n if !KDEVOPS_FIRST_RUN
	help
	  To enable a user to be able to spawn libvirt guests as a regular user
	  a user is typically added to a few groups. These groups are not
	  effective immediately, and so before a user can assume that they
	  use Vagrant they must verify that the required groups are effective.
	  If you enable this option, we will spawn an Ansible role that will
	  verify and ensure that your user is already part of these groups.
	  You can safely say yes here.

choice
	prompt "Libvirt URI"
	default LIBVIRT_URI_SYSTEM if !DISTRO_FEDORA
	default LIBVIRT_URI_SESSION if DISTRO_FEDORA

config LIBVIRT_URI_SYSTEM
	bool "Use qemu:///system for the URI"
	help
	  The first design behind libvirt is to use the system URI, that is,
	  qemu:///system. All 'system' URIs (be it QEMU, lxc, uml, ...)
	  connect to the libvirtd daemon running as root which is launched at
	  system startup. Virtual machines created and run using 'system'
	  are usually launched as root, unless configured otherwise (for
	  example in /etc/libvirt/qemu.conf). A distribution can however still
	  allow users to use the system URI if they are added to the respective
	  groups to use libvirt, and this is the approach taken by kdevops when
	  this option is enabled.

	  You will definitely want to use qemu:///system if your VMs are
	  acting as servers. VM autostart on host boot only works for 'system',
	  and the root libvirtd instance has necessary permissions to use
	  proper networkings via bridges or virtual networks. qemu:///system
	  is generally what tools like virt-manager default to.

	  When this option is enabled vagrant's libvirt default built-in
	  URI is used along with the default network management interface,
	  libvirt socket, and the network interface assumed for bridging.

	  For more details on this refer to the libvirt wiki which still
	  advises in favor of the system URI over the session URI:

	  https://wiki.libvirt.org/page/FAQ#What_is_the_difference_between_qemu:.2F.2F.2Fsystem_and_qemu:.2F.2F.2Fsession.3F_Which_one_should_I_use.3F

config LIBVIRT_URI_SESSION
	bool "Use qemu:///session for the URI"
	help
	  A second design consideration has been implemented into libvirt to
	  enable users to use libvirt without the libvirt daemon needing to
	  run as root. All 'session' URIs launch a libvirtd instance as your
	  local user, and all VMs are run with local user permissions.

	  The benefit of qemu:///session is that permission issues vanish:
	  disk images can easily be stored in $HOME, serial PTYs are owned by
	  the user, etc.

	  qemu:///session has a serious drawback: since the libvirtd instance
	  does not have sufficient privileges, the only out of the box network
	  option is QEMU's usermode networking, which has non obvious
	  limitations, so its usage is discouraged. More info on QEMU
	  networking options: http://people.gnome.org/~markmc/qemu-networking.html
	  With regards to kdevops, if you use the session URI we don't
	  instantiate secondary interfaces with private IP addresses. This is
	  not a requirement for the currently supported workflows but if
	  you are doing custom networking stuff this may be more relevant for
	  you. Fedora defaults to the session URI.

	  When this option is enabled we modify vagrant's libvirt default
	  built-in URI for the session URI, and we also modify the default
	  network management interface to be virbr0, the default socket
	  is assumed to be /run/libvirt/libvirt-sock-ro. New Kconfig options
	  can be added later to customize those further if we really need
	  to.

	  Please note that sensible defaults are enabled for your Linux
	  distribution, so if your distribution does not have session URI
	  set by default it means it doesn't support it yet and you should
	  expect things to not work, and put the work to fix / enhance that
	  somehow. That work likely is not on kdevops... but perhaps this
	  could be wrong. Testing has be done with session support on Debian
	  testing, Ubuntu 21.10 and they both have issues. Don't enable session
	  support manually unless you know what you are doing.

config LIBVIRT_URI_CUSTOM
	bool "Custom QEMU URI"
	help
	  Select this option if you want to manually specify which URI to use.
	  In other words you know what you are doing.

endchoice

config LIBVIRT_URI_PATH
	string "Libvirt QEMU URI to use"
	default "qemu:///system" if LIBVIRT_URI_SYSTEM || LIBVIRT_URI_CUSTOM
	default "qemu:///session" if LIBVIRT_URI_SESSION
	help
	  By default Vagrant uses a qemu:///system URI which assumes the libvirt
	  daemon runs as a user other than the user which is running the vagrant
	  commands. Libvirt has support for running the libvirt daemon as other
	  users using session support. This will be modified to a session URI
	  if you enable LIBVIRT_URI_SESSION. You can however set this to
	  something different to suit your exact needs here. This is the value
	  passed to the vagrant-libvirt plugin libvirt.uri. You should not have
	  to modify this value if you selected LIBVIRT_URI_SYSTEM or
	  LIBVIRT_URI_SESSION and are starting from a fresh 'make mrproper'
	  setting on kdevops, the appropriate value will be set for you.
	  You should only have to modify this manually if you set
	  LIBVIRT_URI_CUSTOM and you know what you are doing.

config LIBVIRT_SYSTEM_URI_PATH
	string "Libvirt system QEMU URI to use"
	default "qemu:///system"
	help
	  This is the URI of QEMU system connection, used to obtain the IP
	  address for management. This is used for the vagrant-libvirt plugin
	  libvirt.system_uri setting. If for whatever reason this needs to
	  be modified you can do so here. Even if you are using session
	  support you should leave this with the default qemu:///system setting
	  as this is still used to ensure your guest's IP address will be
	  communicated back to Vagrant so it determines the guest is up and
	  you can ssh to it. Setting this to qemu:///session still gets the
	  guest up but Vagrant won't know the guest is up, even though the
	  host can ssh to the guest. You should only modify this value if
	  you know what you are doing.

config LIBVIRT_QEMU_GROUP
	string
	default "qemu" if !DISTRO_DEBIAN && !DISTRO_UBUNTU
	default "libvirt-qemu" if DISTRO_DEBIAN || DISTRO_UBUNTU

config KDEVOPS_STORAGE_POOL_PATH
	string
	default LIBVIRT_STORAGE_POOL_PATH_AUTO if LIBVIRT && !LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	default LIBVIRT_STORAGE_POOL_PATH_AUTO if LIBVIRT && LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM if LIBVIRT && LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	default VIRTUALBOX_STORAGE_POOL_PATH_CUSTOM if VAGRANT_VIRTUALBOX

config QEMU_BIN_PATH
	string
	default QEMU_BIN_PATH_LIBVIRT if LIBVIRT
	default "/usr/bin/qemu-system-x86_64" if !LIBVIRT

config LIBVIRT_URI
	string
	default "qemu:///system" if !LIBVIRT
	default LIBVIRT_URI_PATH if LIBVIRT

config LIBVIRT_SYSTEM_URI
	string
	default "qemu:///system" if !LIBVIRT
	default LIBVIRT_SYSTEM_URI_PATH if LIBVIRT

config LIBVIRT_SESSION
	bool
	default LIBVIRT_URI_SESSION

# Only fedora is using this for now. We can add options to modify
# this once this changes or if someone wants to really modify these.
if LIBVIRT_SESSION

config LIBVIRT_SESSION_SOCKET
	string
	default "/run/libvirt/libvirt-sock-ro"

config LIBVIRT_SESSION_MANAGEMENT_NETWORK_DEVICE
	string
	default "virbr0"

config LIBVIRT_SESSION_PUBLIC_NETWORK_DEV
	string
	default "virbr0"

endif # LIBVIRT_SESSION

config USE_LIBVIRT_MIRROR
	bool
	default y if USE_LOCAL_LINUX_MIRROR
	default n if !USE_LOCAL_LINUX_MIRROR

config LIBVIRT_LARGE_CPU
	bool "Enable extremely large CPU count"
	depends on LIBVIRT
	help
	  Select this option if you want to enable larger CPUs
	  over what most KVM / libvirt configurations allow these days.
	  The maximum number of virtual CPUs you can use can vary
	  system to system, but if using KVM one can query KVM using
	  KVM_CHECK_EXTENSION ioctl to /dev/kvm and query for the
	  KVM_CAP_MAX_VCPUS. Even though this has been 1024 since
	  commit 074c82c8f7cf8a ("kvm: x86: Increase MAX_VCPUS to 1024")
	  and this was merged on v5.15 libvirt still uses only max
	  255. You can also enable an ioapic and iommu to bump you to
	  288, but these are software emulated and so are slow and
	  not recommended.

	  We could later add support to make dynconfig support here
	  to dynamically detect you CPU limits. Most distros can
	  use virsh maxvcpus to get the limit but we can later
	  consider just installing kvmtool as well. For now deal with
	  a max 255 by default as that seems to be what most rolling
	  Linux distributions are using...

	  References:

	  https://www.suse.com/support/kb/doc/?id=000019723
	  https://lwn.net/Articles/658511/
	  https://git.kernel.org/pub/scm/linux/kernel/git/will/kvmtool.git/

choice
	prompt "Guest vCPUs"
	default LIBVIRT_VCPUS_8

config LIBVIRT_VCPUS_2
	bool "2"
	help
	  Use 2 vCPUs on guests.

config LIBVIRT_VCPUS_4
	bool "4"
	help
	  Use 4 vCPUs on guests.

config LIBVIRT_VCPUS_8
	bool "8"
	help
	  Use 8 vCPUs on guests.

config LIBVIRT_VCPUS_16
	bool "16"
	help
	  Use 16 vCPUs on guests.

config LIBVIRT_VCPUS_32
	bool "32"
	help
	  Use 32 vCPUs on guests.

config LIBVIRT_VCPUS_64
	bool "64"
	help
	  Use 64 vCPUs on guests.

config LIBVIRT_VCPUS_128
	bool "128"
	help
	  Use 128 vCPUs on guests.

config LIBVIRT_VCPUS_255
	bool "255"
	help
	  Use 255 vCPUs on guests.

config LIBVIRT_VCPUS_288
	bool "288"
	depends on LIBVIRT_LARGE_CPU
	help
	  Use 288 vCPUs on guests.

config LIBVIRT_VCPUS_512
	bool "512"
	depends on LIBVIRT_LARGE_CPU
	help
	  Use 512 vCPUs on guests.

endchoice

config LIBVIRT_VCPUS_COUNT
	int
	default 2 if LIBVIRT_VCPUS_2
	default 4 if LIBVIRT_VCPUS_4
	default 8 if LIBVIRT_VCPUS_8
	default 16 if LIBVIRT_VCPUS_16
	default 32 if LIBVIRT_VCPUS_32
	default 64 if LIBVIRT_VCPUS_64
	default 128 if LIBVIRT_VCPUS_128
	default 255 if LIBVIRT_VCPUS_255
	default 288 if LIBVIRT_VCPUS_288
	default 512 if LIBVIRT_VCPUS_512
	help
	  The number of virtual CPUs to use per guest.

choice
	prompt "How much GiB memory to use per guest"
	default LIBVIRT_MEM_4G

config LIBVIRT_MEM_2G
	bool "2"
	help
	  Use 2 GiB of RAM on guests. Most workflows should work well
	  except for:
	  - xfs/074 is known to fail due to the amount of RAM used by the
	    obsolete xfs_scratch. Since xfs_scratch is obsolete this test
	    will not run by default unless the test runner uses
	    FORCE_XFS_CHECK_PROG=yes
	  - git cloning linux-next requires > 2 GiB RAM if your clone is not
	    shallow (BOOTLINUX_SHALLOW_CLONE)
	  - xfs/084 and generic/627 often get killed by OOM killer and skipped

config LIBVIRT_MEM_3G
	bool "3"
	help
	  Use 3 GiB of RAM on guests. No OOM killed tests observed when using
	  3 GiB of RAM on most workflows.

config LIBVIRT_MEM_4G
	bool "4"
	help
	  Use 4 GiB of RAM on guests. No known issues are known when using
	  4 GiB of RAM on most workflows.

config LIBVIRT_MEM_8G
	bool "8"
	help
	  Use 8 GiB of RAM on guests.

config LIBVIRT_MEM_16G
	bool "16"
	help
	  Use 16 GiB of RAM on guests.

config LIBVIRT_MEM_32G
	bool "32"
	help
	  Use 32 GiB of RAM on guests.

endchoice

config LIBVIRT_MEM_MB
	int
	default 2048 if LIBVIRT_MEM_2G
	default 3072 if LIBVIRT_MEM_3G
	default 4096 if LIBVIRT_MEM_4G
	default 8192 if LIBVIRT_MEM_8G
	default 16384 if LIBVIRT_MEM_16G
	default 32768 if LIBVIRT_MEM_32G
	help
	  How much MiB of RAM to use per guest.

config HAVE_LIBVIRT_PCIE_PASSTHROUGH
	bool
	default $(shell, scripts/check_pciepassthrough_kconfig.sh passthrough_libvirt.generated)

if HAVE_LIBVIRT_PCIE_PASSTHROUGH
source "vagrant/Kconfig.pcie_passthrough_libvirt"
endif # HAVE_LIBVIRT_PCIE_PASSTHROUGH

choice
	prompt "Machine type to use"
	default LIBVIRT_MACHINE_TYPE_Q35

config LIBVIRT_MACHINE_TYPE_DEFAULT
	bool "Use the default machine type"
	help
	  Use whatever default the guest was intended to use. This will either
	  be the machine type used at virt-install time or some other default
	  by QEMU / libvirt. This is important for backward compatibility with
	  older kernels. For example if you enable q35 on an old kernel the old
	  kernel may not boot. For details refer to kdevops commit 83952e2e532e
	  ("vagrant/kdevops_nodes.yaml.in: remove machine_type)".

	  It would seem today's default for libvirt is to still use "pc" which
	  is the old i440x, so you will not get PCIe support.

config LIBVIRT_MACHINE_TYPE_Q35
	bool "q35"
	help
	  Use q35 for the machine type. This will be required for things like
	  CXL or PCIe passthrough.

endchoice

config LIBVIRT_HOST_PASSTHROUGH
	bool "Use CPU host-passthrough"
	default y
	help
	  Enable this to be able to also carry the same CPU your host has to
	  the guest. As per QEMU documentation this is the recommended CPU
	  type to use, provided live migration is not required. And we're
	  not supporting live-migration on kdevops so this our default too.
	  This will also enable you to use things like perf stat and
	  get access to some PMUs:

	  perf stat --repeat 2 -e \
	    dTLB-loads,dTLB-load-misses,dTLB-stores,dTLB-stores-misses,\
	    iTLB-loads,iTLB-load-misses,\
	    itlb_misses.walk_completed_4k\
	    itlb_misses.walk_completed_2m_4m\
	    page-faults,tlb:tlb_flush  \
	      --pre 'make -s mrproper defconfig' \
	    \-- make -s -j$(nproc) bzImage

config QEMU_BUILD
	bool "Should we build QEMU for you?"
	select NEEDS_LOCAL_DEVELOPMENT_PATH
	help
	  You only want to enable this option if your distribution package
	  of QEMU does not have support for the features you need. For
	  example this may be useful if you are a QEMU developer or are
	  relying on technology is still under development or if you have
	  a custom QEMU git URL.

if !QEMU_BUILD

config QEMU_USE_DEVELOPMENT_VERSION
	bool "Should we look for a development version of QEMU?"
	help
	  You want to enable this option if for example the currently
	  available version of QEMU does not yet have support for the feature
	  you are going to be working on.

	  Say yes here if you are compiling your own version of QEMU.

endif # !QEMU_BUILD

if QEMU_BUILD

choice
	prompt "QEMU git URL to use"
	default QEMU_BUILD_JIC23

config QEMU_BUILD_UPSTREAM
	bool "https://gitlab.com/qemu-project/qemu.git"
	help
	  Select this option if you want to use the upstream QEMU git repo.

config QEMU_BUILD_JIC23
	bool "https://gitlab.com/jic23/qemu.git"
	help
	  Select this option if you want to use Cameron's QEMU git repo.
	  This has a few CXL bells and whistles which are not yet upstream.

config QEMU_BUILD_MANUAL
	bool "Custom QEMU git URL"
	help
	  Select this option if you want to specify your own git URL.

endchoice

config QEMU_BUILD_GIT
	string "Git tree for QEMU to clone on localhost"
	default "/mirror/qemu.git" if USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default "/mirror/qemu-jic23.git" if USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_JIC23
	default "https://gitlab.com/qemu-project/qemu.git" if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_UPSTREAM || QEMU_BUILD_MANUAL
	default "https://gitlab.com/jic23/qemu.git" if !USE_LOCAL_LINUX_MIRROR && QEMU_BUILD_JIC23
	help
	  This is the git URL to use to clone and then build QEMU for you on
	  your localhost.

config QEMU_BUILD_GIT_DATA_PATH
	string "The destination directory where to clone the QEMU git tree"
	default "{{local_dev_path}}/qemu"
	help
	  This is the target location of where to clone the above git tree.
	  Note that {{local_dev_path}} corresponds to the location set by the
	  configuration option CONFIG_NEEDS_LOCAL_DEVELOPMENT_PATH.

config QEMU_BUILD_GIT_VERSION
	string "The version of QEMU to build"
	default "cxl-2023-05-19" if QEMU_BUILD_JIC23
	default "v8.0.0-rc1" if QEMU_BUILD_UPSTREAM
	help
	  This is the target build version of QEMU to build. Please use
	  at least v7.2.0 for CXL support. v8.0.0-rc1 has some build fixes
	  for newer compilers so it is the default now.

config QEMU_USE_DEVELOPMENT_VERSION
	bool
	default y

endif # QEMU_BUILD

config QEMU_BIN_PATH_LIBVIRT
	string "QEMU binary path to use"
	default "/usr/local/bin/qemu-system-x86_64" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin/qemu-system-x86_64" if !QEMU_USE_DEVELOPMENT_VERSION

config QEMU_INSTALL_DIR_LIBVIRT
	string "Path to install QEMU"
	default "/usr/local/bin" if QEMU_USE_DEVELOPMENT_VERSION
	default "/usr/bin" if !QEMU_USE_DEVELOPMENT_VERSION

config QEMU_VIRSH_CAN_SUDO
	bool
	default $(shell, ./scripts/get_libvirsh_can_sudo.sh)

choice
	prompt "Libvirt extra storage driver to use"
	default LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO

config LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	bool "NVMe"
	help
	  Use the QEMU NVMe driver for extra storage drives. We always expect
	  to use this as we expect *could* be outperforming the virtio driver.
	  Only if you enable this will you get support for ZNS too. We expect
	  the NVMe driver to always be the default. It also gives us parity with
	  cloud bringups.

config LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	bool "virtio"
	help
	  Use the QEMU virtio driver for extra storage drives. Use this if you
	  are having issues with "NVMe timeouts" issues when testing in a loop
	  with fstests and cannot upgrade your QEMU version. If you select this
	  you won't be able to test ZNS.

config LIBVIRT_EXTRA_STORAGE_DRIVE_IDE
	bool "ide"
	help
	  Use the QEMU ide driver for extra storage drives. This is useful for
	  really old Linux distributions that lack the virtio backend driver.

endchoice

choice
	prompt "QEMU NVMe extra storage logical block size"
	default LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	help
	  The logical block size to use for extra NVMe drives. This ends up
          what is put into the /sys/block/<disk>/queue/logical_block_size (and
	  /sys/block/<disk>/queue/physical_block_size) the smallest unit the
          storage device can address. It is typically 512 bytes.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512
	bool "512 bytes"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_NVME_LOGICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "QEMU virtio extra storage physical block size"
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	default LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512
	help
	  The physical block size to use for extra drive. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512
	bool "512 bytes"
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_PHYSICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "QEMU virtio extra storage logical block size"
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_VIRTIO
	default LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512
	help
	  The logical block size to use for extra drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512
	bool "512 bytes"
	select EXTRA_STORAGE_SUPPORTS_512
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  512 bytes logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_1K
	bool "1 KiB"
	select EXTRA_STORAGE_SUPPORTS_1K
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  1 KiB logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_2K
	bool "2 KiB"
	select EXTRA_STORAGE_SUPPORTS_2K
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  2 KiB (2048 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_4K
	bool "4 KiB"
	select EXTRA_STORAGE_SUPPORTS_4K
	help
	  4 KiB (4096 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_8K
	bool "8 KiB"
	help
	  8 KiB (8192 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_16K
	bool "16 KiB"
	help
	  16 KiB (16384 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_32K
	bool "32 KiB"
	help
	  32 KiB (32768 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_64K
	bool "64 KiB"
	help
	  64 KiB (65536 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_128K
	bool "128 KiB"
	help
	  128 KiB (131072 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_256K
	bool "256 KiB"
	help
	  256 KiB (262144 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_512K
	bool "512 KiB"
	help
	  512 KiB (524288 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_1M
	bool "1 MiB"
	help
	  1 MiB (1048576 bytes) logical block size.

config LIBVIRT_EXTRA_STORAGE_VIRTIO_LOGICAL_BLOCK_SIZE_2M
	bool "2 MiB"
	help
	  2 MiB (2097152 bytes) logical block size.

endchoice

choice
	prompt "Libvirt aio mode"
	default LIBVIRT_AIO_MODE_IO_URING

config LIBVIRT_AIO_MODE_NATIVE
	bool "aio=native"
	depends on !KDEVOPS_LIBVIRT_PCIE_PASSTHROUGH
	help
	  Use the aio=native mode. For some older kernels it is known that
	  native will cause corruption if used on ext4 or xfs filesystem if
	  you also use cace=none. This corruption is documented for RHEL:

	  https://access.redhat.com/articles/41313
	  https://bugzilla.redhat.com/show_bug.cgi?id=615309

	  In terms of performance there are some recommendations to not use
	  aio=native on sparsefiles. The claim seems to be that a host
	  filesystems metadata write can block the AIO io_submit() call and
	  therefore block QEMU threads which expect AIO behaviour on the guest.
	  This is documented when on Openstack nova an aio mode option was
	  requested to be available for different backends:

	  If you want to use PCIe passthrough you cannot use aio=native.

	  https://review.opendev.org/c/openstack/nova-specs/+/232514/7/specs/mitaka/approved/libvirt-aio-mode.rst

config LIBVIRT_AIO_MODE_THREADS
	bool "aio=threads"
	help
	  Use the aio=threads mode. This might be more suitable for you if on
	  older kernels such as in RHEL6 and using sparsefiles and ext4 or xfs
	  on this host with cache=none.

config LIBVIRT_AIO_MODE_IO_URING
	bool "aio=io_uring"
	help
	  Use the aio=io_uring mode. This is currently experimental.
endchoice

config LIBVIRT_AIO_MODE
	string
	default "native" if LIBVIRT_AIO_MODE_NATIVE
	default "threads" if LIBVIRT_AIO_MODE_THREADS
	default "io_uring" if LIBVIRT_AIO_MODE_IO_URING

choice
	prompt "Libvirt cache mode"
	default LIBVIRT_AIO_CACHE_MODE_NONE

config LIBVIRT_AIO_CACHE_MODE_NONE
	bool "cache=none"
	help
	  Use the cache=none. IO from the guest is not cached on the host but
	  it may be kept in a writeback disk cache. This means that the actual
	  storage device may report a write as completed when the data is still
	  placed in the host's write queue only, the guest's virtual storage
	  adapter is informed that there is a writeback cache. So in essence
	  the guest's writes are directly accessing the host's disk. Use this
	  option for guests with large IO requirements. This is generally the
	  best option and is required for live migration.

config LIBVIRT_AIO_CACHE_MODE_WRITETHROUGH
	bool "cache=writethrough"
	help
	  cache=writethrough. IO from the guest is cached on the host but
	  written through to the physical medium. Writes are only reported as
	  completed when the data has been committed to the storage device. The
	  guest's virtual storage adapter is informed that there is no
	  writeback cache so the guest does not need to send flush commands
	  to manage integrity. This mode is slower and prone to scaling issues.
	  Best used for small number of guests with lower IO reqs. This should
	  be used on older guests which do not support writeback cache.

config LIBVIRT_AIO_CACHE_MODE_WRITEBACK
	bool "cache=writeback"
	help
	  cache=writeback. IO from the guest is cached on the host so it is
	  cached on the host's page cache. Writes are reported to the guest
	  when they are placed on the host's page cache. The guest's virtual
	  storage controller is informed of the writeback cache and therefore
	  expected to send flush commands as needed to manage data integrity.

config LIBVIRT_AIO_CACHE_MODE_DIRECTSYNC
	bool "cache=directsync"
	help
	  cache=directsync. Writes are reported only when the data has been
	  committed to the storage controller. The host cache is completely
	  bypassed. This mode is useful for guests which that do not send
	  flushes when needed.

config LIBVIRT_AIO_CACHE_MODE_UNSAFE
	bool "cache=unsafe"
	help
	  cache=unsafe. Similar to writeback except all of the flush commands
	  from the guest are ignored. This is useful if the one wants to
	  prioritize performance and one does not care about data loss.

endchoice

config LIBVIRT_AIO_CACHE_MODE
	string
	default "none" if LIBVIRT_AIO_CACHE_MODE_NONE
	default "writethrough" if LIBVIRT_AIO_CACHE_MODE_WRITETHROUGH
	default "writeback" if LIBVIRT_AIO_CACHE_MODE_WRITEBACK
	default "directsync" if LIBVIRT_AIO_CACHE_MODE_DIRECTSYNC
	default "unsafe" if LIBVIRT_AIO_CACHE_MODE_UNSAFE

choice
	prompt "Libvirt drive file format"
	depends on LIBVIRT
	default LIBVIRT_EXTRA_DRIVE_FORMAT_RAW

config LIBVIRT_EXTRA_DRIVE_FORMAT_QCOW2
	bool "Use qcow2 format"
	help
	  Select this option if you want to use the qcow2 file format for the
	  extra storage drives created for you. This is useful if you want to use
	  advanced features on the files such as growing them or freezing
	  them. There may be some odd issues however with very sub-optimal
	  features such as with discard, however these issues are still being
	  investigated.

config LIBVIRT_EXTRA_DRIVE_FORMAT_RAW
	bool "Use raw format"
	help
	  Select this option if you want to use the raw file format for the extra storage
	  drives created for you. One of the sweet spots for using raw and 4KiB
	  block sizes is the advantages of ensuring that when a filesystem
	  issues a punch hole through fallocate (FALLOC_FL_PUNCH_HOLE) write
	  zeroes (REQ_OP_WRITE_ZEROES) results in an actual deallocation of
	  blocks faster. When using qcow2 the default is to use a cluster of
	  64 KiB for blocks and by default blocks will only be marked
	  deallocated if a full cluster is zeroed or discarded. You can
	  fallocate (punch) holes on a filesystem with a smaller block
	  size than the default qcow2 cluster size (say 4 KiB), and so in
	  theory, this may cause undeterministic delays. In practice this may
	  end up in NVMe timeouts, but these issues are currently being
	  investigated, and if using the raw format proves to not cause NVMe
	  timeouts as observed with fstests punch tests while the backend
	  drive has low space (below 80%) this may become the default for
	  kdevops as it reflects a possible bug in QEMU.

endchoice

choice
	prompt "Libvirt storage pool path"
	default LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED if DISTRO_DEBIAN && QEMU_VIRSH_CAN_SUDO
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO if !DISTRO_SUSE && !DISTRO_DEBIAN
	default LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD if DISTRO_SUSE

config LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	bool "Use an advanced smart inference for what storage pool path to use"
	help
	  If you are a power user of kdevops you likely want to enable this.
	  By default vagrant will assume that if you don't have a virsh pool
	  that the current directory will become the "default" storage pool
	  path. This is rather silly for an advanced setup. Consider a setup
	  where you have a set of different NVMe drivers mounted on different
	  partitions:

	  /dev/nvme0n1 --> /data1 with xfs
	  /dev/nvme1n1 --> /data2 with btrfs
	  /dev/nvme2n1 --> /data2 with ext4

	  In this setup, if you have a kdevops instance initialized in
	  /data1/ somewhere you likely want to use a virsh pool path under
	  /data1/libvirt/images/ so that all local requests to that drive
	  go to that storage pool that. By enabling this option kdevops will
	  infer this information for you and figure out what storage pool path
	  to use. It will scrape your existing virsh pool-list and use the first
	  path where the first directory of your current working directory
	  lies.

	  This is enabled by default on Debian only now as this has been
	  tested there first. If this gets tested on other distributions
	  they should migrate over to enable power users to be more lazy
	  on initial bringups.

	  Eventually this can be the default but it requires high confidence
	  that the heuristics will work on each distro with their own defaults.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	bool "Use the current vagrant working directory"
	help
	  Select this option if you want to use the vagrant directory inside
	  where you git cloned kdevops as the libvirt storage pool path where
	  we'll download images and store images for guests spawned. If users
	  git cloned kdevops somewhere in their home directory they'll have to
	  make sure that the group which libvirt is configured to run for their
	  distribution can have access to that directory. As it is today it
	  would seem only fedora restricts the $HOME to g-rwx o-rwx and so
	  by default this option won't work on Fedora by default. This used
	  to be the old default on kdevops but because of this it should not
	  be used by default. Distributions still relying on this should
	  verify they can change this default.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	bool "Use the same path typically used by each distribution"
	help
	  Select this option if you want to use the same location as the
	  distribution would typically use. We expect this to be
	  /var/lib/libvirt/images/ for most distributions, however we can
	  customize this further if this is not true by adding further checks.

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL
	bool "Custom manual path"
	help
	  Select this option if you want to manually specify where to use as
	  the QEMU storage pool path. This is today's default given otherwise
	  we may have to muck with the $HOME directory permissions.

endchoice

config LIBVIRT_STORAGE_POOL_PATH_AUTO
	string
	default $(shell, ./scripts/get_libvirsh_pool_path.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default "/var/lib/libvirt/images" if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_DEFAULT_DISTRO
	default $(shell, scripts/cwd-append.sh vagrant) if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_CWD
	help
	  The path to use for the libvirt storage pool path. Since kdevops uses
	  Vagrant for virtualization this is also the path used to place the
	  additional NVMe drives created. kdevops adds a postfix "kdevops" to
	  this directory as it wants to allow Vagrant full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will let Vagrant store images in /var/lib/libvirt/images/ and
	  the NVMe qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/.vagrant/nvme_disks/guest-hostname/.

if LIBVIRT_STORAGE_POOL_PATH_CUSTOM_MANUAL

config LIBVIRT_STORAGE_POOL_PATH_CUSTOM
	string "Custom libvirt storage pool location"
	default "/var/lib/libvirt/images"
	help
	  The path to use for the libvirt storage pool path. Since kdevops uses
	  Vagrant for virtualization this is also the path used to place the
	  additional NVMe drives created. kdevops adds a postfix "kdevops" to
	  this directory as it wants to allow Vagrant full control over that
	  directory. For instance if this is /var/lib/libvirt/images/ kdevops
	  will let Vagrant store images in /var/lib/libvirt/images/ and
	  the NVMe qcow2 files created will go in by default to the directory
	  /var/lib/libvirt/images/kdevops/.vagrant/nvme_disks/guest-hostname/.

endif

config LIBVIRT_STORAGE_POOL_CREATE
	bool "Should we build a custom storage pool for you?"
	default n if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_enabled.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  By default vagrant assumes your storage pool name is "default" and
	  it expects libvirt to have created this for you. If you want to
	  use a custom pool name and path enable this. This is useful if
	  you want to place guest images on another path other than the
	  default libvirt has setup for you on the "default" pool name.

config LIBVIRT_STORAGE_POOL_NAME
	string "Libvirt storage pool name"
	depends on LIBVIRT_STORAGE_POOL_CREATE
	default "default" if !LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	default $(shell, ./scripts/get_libvirsh_pool_name.sh) if LIBVIRT_STORAGE_POOL_PATH_INFER_ADVANCED
	help
	  The libvirt storage pool name to use. By default this is "default",
	  and this is typically defined by libvirt. Even if you create guests
	  on a separate directory unless you use a custom storage pool name
	  here the default path used where libvirt created the default
	  storage pool path will be used for data. You should modify the
	  storage pool name to something other than "default" here if you
	  are using a custom storage pool path and are doing so to ensure
	  you don't waste space in whatever path libvirt's default storage
	  pool path is set to. To check your libvirt's default storage pool
	  path you can run this and look for the path:

	    virsh pool-dumpxml default

	  You will need to use sudo on all distros which do not use
	  LIBVIRT_URI_SESSION (so all distros other than Fedora).
	  If you set here something other than 'default' kdevops will create
	  this pool upon 'make bringup' if the pool is not yet available. It
	  will do this with:

	    virsh pool-define-as NAME PATH
	    virsh pool-start NAME
	    virsh pool-autostart NAME

	  That name you use here is up to you but you should use something
	  which would make it obvious what it is for other users on the system.
	  For instance you may want to use a volume name of "data2" for a path
	  on a partition on /data2/ or something like that.

config QEMU_ENABLE_NVME_ZNS
	bool "Enable QEMU NVMe ZNS drives"
	depends on LIBVIRT && LIBVIRT_EXTRA_STORAGE_DRIVE_NVME
	default n
	help
	  If this option is enabled then you can enable NVMe ZNS drives on the
	  guests.

config QEMU_CUSTOM_NVME_ZNS
	bool "Customize QEMU NVMe ZNS settings"
	depends on QEMU_ENABLE_NVME_ZNS
	default n
	help
	  If this option is enabled then you will be able to modify the defaults
	  used for the 2 NVMe ZNS drives we create for you. By default we create
	  two NVMe ZNS drives with 100 GiB of total size, each zone being
	  128 MiB, and so you end up with 800 total zones. The zone capacity
	  equals the zone size. The default zone size append limit is also
	  set to 0, which means the zone append size limit will equal to the
	  maximum data transfer size (MDTS). The default logical and physical
	  block size of 4096 bytes is also used. If you want to customize any
	  of these ZNS settings for the drives we bring up enable this option.

	  If unsure say N.

if QEMU_CUSTOM_NVME_ZNS

config QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE
	int "QEMU ZNS storage NVMe drive size"
	default 102400
	help
	  The size of the QEMU NVMe ZNS drive to expose. We expose 2 NVMe
	  ZNS drives of 100 GiB by default. This value chagnes its size.
	  100 GiB is a sensible default given most full fstests require about
	  50 GiB of data writes.

config QEMU_CUSTOM_NVME_ZONE_ZASL
	int "QEMU ZNS zasl - zone append size limit power of 2"
	default 0
	help
	  This is the zone append size limit. If left at 0 QEMU will use
	  the maximum data transfer size (MDTS) for the zone size append limit.
	  Otherwise if this value is set to something other than 0, then the
	  zone size append limit will equal to 2 to the power of the value set
	  here multiplied by the minimum memory page size (4096 bytes) but the
	  QEMU promises this value cannot exceed the maximum data transfer size.

config QEMU_CUSTOM_NVME_ZONE_SIZE
	string "QEMU ZNS storage NVMe zone size"
	default "128M"
	help
	  The size the the QEMU NVMe ZNS zone size. The number of zones are
	  implied by the driver size / zone size. If there is a remainder
	  technically that should go into another zone with a smaller zone
	  capacity.

config QEMU_CUSTOM_NVME_ZONE_CAPACITY
	string "QEMU ZNS storage NVMe zone capacity"
	default "0M"
	help
	  The size to use for the zone capacity. This may be smaller or equal
	  to the zone size. If set to 0 then this will ensure the zone
	  capacity is equal to the zone size.

config QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE
	int "QEMU ZNS storage NVMe zone max active"
	default 0
	help
	  The max numbe of active zones. The default of 0 means all zones
	  can be active at all times.

config QEMU_CUSTOM_NVME_ZONE_MAX_OPEN
	int "QEMU ZNS storage NVMe zone max open"
	default 0
	help
	  The max numbe of open zones. The default of 0 means all zones
	  can be opened at all times. If the number of active zones is
	  specified this value must be less than or equal to that value.

config QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCK_SIZE
	int "QEMU ZNS storage NVMe physical block size"
	default 4096
	help
	  The physical block size to use for ZNS drives. This ends up
	  what is put into the /sys/block/<disk>/queue/physical_block_size
	  and is the smallest unit a physical storage device can write
	  atomically. It is usually the same as the logical block size but may
	  be bigger. One example is SATA drives with 4KB sectors that expose a
	  512-byte logical block size to the operating system. For stacked
	  block devices the physical_block_size variable contains the maximum
	  physical_block_size of the component devices.

config QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCK_SIZE
	int "QEMU ZNS storage NVMe logical block size"
	default 4096
	help
	  The logical block size to use for ZNS drives. This ends up what is
	  put into the /sys/block/<disk>/queue/logical_block_size and the
	  smallest unit the storage device can address. It is typically 512
	  bytes.

endif # QEMU_CUSTOM_NVME_ZNS

config LIBVIRT_ENABLE_ZNS
	bool
	default y if QEMU_ENABLE_NVME_ZNS

config QEMU_NVME_ZONE_DRIVE_SIZE
	int
	default 102400 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_DRIVE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_ZASL
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_ZASL if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_SIZE
	string
	default "128M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_CAPACITY
	string
	default "0M" if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_CAPACITY if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_ACTIVE
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_ACTIVE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_MAX_OPEN
	int
	default 0 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_MAX_OPEN if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_PHYSICAL_BLOCK_SIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_PHYSICAL_BLOCK_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_NVME_ZONE_LOGICAL_BLOCK_SIZE
	int
	default 4096 if !QEMU_CUSTOM_NVME_ZNS
	default QEMU_CUSTOM_NVME_ZONE_LOGICAL_BLOCK_SIZE if QEMU_CUSTOM_NVME_ZNS

config QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	bool "Enable QEMU drives for large IO experimentation"
	depends on LIBVIRT
	default n
	help
	  If you want to experiment with large IO either with NVMe or virtio
	  you can enable this option. This will create a few additional drives
	  which are dedicated for largio experimentation testing.

	  For now you will need a distribution with a root filesystem on XFS
	  or btrfs, and so you will want to enable the kdevops distribution and
	  VAGRANT_KDEVOPS_DEBIAN_TESTING64_XFS_20230427. This is a requirement
	  given all block devices must use iomap and that is the only current
	  way to disable buffer-heads. Eventually this limitation is expected
	  You can also use large-block-20230525 with Amazon Linux 2023 on AWS.

	  If unsure say N.

if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_EXTRA_DRIVE_LARGEIO_NUM_DRIVES_PER_SPACE
	int "How many qemu drives to create per each target size"
	default 4
	help
	  If you are going to try to mess with LBS on 4k LBA you can experiment
	  with:

	  - 4k block size
	  - 8k block size
	  - 16k block size
	  - 32k block size
	  - 64k block size

	  So in total 4 drives. For a drive with an LBA format of 16k, you can
	  only experiment with block sizes:

	  - 16k block size
	  - 32k block size
	  - 64k block size

	  In theory you can experiment up to MAX_PAGECACHE_ORDER and to make
	  things worse some filesystems can use block sizes which are not power
	  of two. For now filesystems only support up to max block size 64k, so
	  we can just keep the max drive sizes down a bit. Likewise twice the
	  PAGE_SIZE is not supported as we require at least order 2 so 16k as
	  folios use the 3rd page for the deferred list. So you really only need
	  for 4k today:

	  - 4k block size
	  - 16k block size
	  - 32k block size
	  - 64k block size

	  If we create 4 drives per space you can have 4 for basic baseline
	  coverage testing. It seems the max limit is about 20 drives per
	  qemu pcie port today, if you enable more than the default 4, good
	  luck!

config QEMU_EXTRA_DRIVE_LARGEIO_BASE_SIZE
	int "QEMU extra drive drive base size"
	default 10240
	help
	  The base size of the QEMU extra storage drive to expose. The
	  size is increased by 1 MiB as we go down the list of extra large IO
	  drives.

config QEMU_EXTRA_DRIVE_LARGEIO_COMPAT
	bool "Use a compatibility logical block size"
	default n
	help
	  Since older spindle drives used to work with 512 bytes some drives
	  exist with support to handle 512 writes even if they physically store
	  more data on their drives for that one 512 byte write. Enable this if
	  you want to ensure your large IO drives always have a logical block
	  size restrained by the compatibility size you want to support.

	  By default this is not enabled, and therefore the logical block size
	  for the large IO drives will be equal to the physical block size.

config QEMU_EXTRA_DRIVE_LARGEIO_COMPAT_SIZE
	int "Large IO compat size"
	default 512
	help
	  This is the compatibility base block size to use for older drives.
	  Even if you disable QEMU_EXTRA_DRIVE_LARGEIO_COMPAT, this value will
	  be used as the base for the computation for the physical block size
	  for the large IO drives we create for you using the formula:

	    libvirt_largeio_logical_compat_size  * (2 ** n)

	  where n is the index of the large IO drive.

config QEMU_EXTRA_DRIVE_LARGEIO_MAX_POW_LIMIT
	int "Large IO - number of drives - power"
	default 7
	help
	  We use an iterator to create the number of large drives on the
	  guest system using:

	    for n in range(0,libvirt_largeio_pow_limit)
	       pbs = compat_size * (2 ** n)

	  Using a compat_size of 512 means we go up to 64k physical block
	  size by using the default of 7.

	  This provides the value for the libvirt_largeio_pow_limit. By
	  default we set this to 12 so we get drives of different physical
	  sizes in powers of 2 ranging from 512 up to 1 GiB. You can reduce
	  this if you want less drives to experiment with.

endif # QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config LIBVIRT_ENABLE_LARGEIO
	bool
	default y if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_DRIVE_BASE_SIZE
	int
	default 10240 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_BASE_SIZE if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_COMPAT_SIZE
	int
	default 512 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_COMPAT_SIZE if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_LARGEIO_MAX_POW_LIMIT
	int
	default 12 if !QEMU_ENABLE_EXTRA_DRIVE_LARGEIO
	default QEMU_EXTRA_DRIVE_LARGEIO_MAX_POW_LIMIT if QEMU_ENABLE_EXTRA_DRIVE_LARGEIO

config QEMU_ENABLE_CXL
	bool "Enable QEMU CXL devices"
	depends on LIBVIRT
	depends on LIBVIRT_MACHINE_TYPE_Q35
	depends on QEMU_USE_DEVELOPMENT_VERSION
	default n
	help
	  If this option is enabled then you can enable different types of
	  CXL devices which we will emulate for you.

if QEMU_ENABLE_CXL

config QEMU_START_QMP_ON_TCP_SOCKET
	bool "Start QMP on a TCP socket"
	default n

if QEMU_START_QMP_ON_TCP_SOCKET

config QEMU_QMP_COMMAND_LINE_STRING
	string "Qemu command line string for qmp"
	default "tcp:localhost:4444,server"
	help
	  Option for qmp interface (from https://wiki.qemu.org/Documentation/QMP).

config QEMU_QMP_WAIT_ON
	bool "Let Qemu instance wait for qmp connection"
	default n

endif # QEMU_START_QMP_ON_TCP_SOCKET

choice
	prompt "CXL topology to enable"
	default QEMU_ENABLE_CXL_DEMO_TOPOLOGY_1

config QEMU_ENABLE_CXL_DEMO_TOPOLOGY_1
	bool "Basic CXL demo topology with a CXL Type 3 device"
	help
	  This is a basic CXL demo topology. It consists of single host bridge that
	  has one root port. A Type 3 persistent memory device is attached to the
	  root port. This topology is referred to as a passthrough decoder in
	  kernel terminology. The kernel CXL core will consume the resource exposed
	  in the ACPI CXL memory layout description, such as Host Managed
	  Device memory (HDM), CXL Early Discovery Table (CEDT), and the
	  CXL Fixed Memory Window Structures to publish the root of a
	  cxl_port decode hierarchy to map regions that represent System RAM,
	  or Persistent Memory regions to be managed by LIBNVDIMM.

config QEMU_ENABLE_CXL_DEMO_TOPOLOGY_2
	bool "Host bridge with two root ports"
	help
	  This topology extends the first demo topology by placing two root ports
	  in the host bridge. This ensures that the decoder associated with the
	  host bridge is not a passthrough decoder.

config QEMU_ENABLE_CXL_SWITCH_TOPOLOGY_1
	bool "CXL switch connected to root port with two down stream ports"
	help
	  This topology adds a CXL switch in the topology. A memory device
	  is connected to one of the down stream ports. The upstream port
	  is connected to a root port on the host bridge.

config QEMU_ENABLE_CXL_DEMO_DCD_TOPOLOGY_1
	bool "CXL DCD demo directly attached to a single-port HB"
	help
	  This topology adds a DCD device in the topology, directly attached to
	  a host bridge with only one root port.
	  The device has zero (volatile or non-volatile) static capacity
	  and 2 dynamic capacity regions where dynamic extents can be added.

endchoice

endif # QEMU_ENABLE_CXL
